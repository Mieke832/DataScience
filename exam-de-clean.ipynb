def load_datasets():
    """
    Load all required datasets with proper data type handling.
    
    Returns:
        tuple: (df_author, df_user, df_daily_user) - loaded DataFrames
    """
    try:
        # Load author interaction statistics
        df_author = pd.read_csv("author_interaction_stats.csv.gz")
        
        # Load user interaction statistics  
        df_user = pd.read_csv("user_interaction_stats.csv.gz")
        
        # Load daily user post statistics with date parsing
        df_daily_user = pd.read_csv("user_post_stats_per_day.csv.gz", 
                                   parse_dates=["date"])
        
        print(f"âœ“ Datasets loaded successfully:")
        print(f"  - Author interactions: {df_author.shape}")
        print(f"  - User interactions: {df_user.shape}")
        print(f"  - Daily user posts: {df_daily_user.shape}")
        
        return df_author, df_user, df_daily_user
        
    except FileNotFoundError as e:
        print(f"âŒ Dataset not found: {e}")
        return None, None, None
    except Exception as e:
        print(f"âŒ Error loading datasets: {e}")
        return None, None, None


def validate_dataframe(df, name, expected_columns=None):
    """
    Validate DataFrame structure and provide summary information.
    
    Args:
        df (pd.DataFrame): DataFrame to validate
        name (str): Name for reporting
        expected_columns (list): Optional list of expected columns
    """
    print(f"\nðŸ“Š {name} Summary:")
    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    if expected_columns:
        missing_cols = set(expected_columns) - set(df.columns)
        if missing_cols:
            print(f"âš ï¸  Missing columns: {missing_cols}")
        else:
            print("âœ“ All expected columns present")
    
    # Check for missing values
    missing_counts = df.isnull().sum()
    if missing_counts.sum() > 0:
        print("Missing values:")
        for col, count in missing_counts[missing_counts > 0].items():
            print(f"  - {col}: {count} ({count/len(df)*100:.1f}%)")
    else:
        print("âœ“ No missing values")


def create_daily_aggregation(df_daily_user):
    """
    Create daily aggregation statistics from user daily data.
    
    Args:
        df_daily_user (pd.DataFrame): Daily user post statistics
        
    Returns:
        pd.DataFrame: Aggregated daily statistics with date index
    """
    # Calculate weighted sentiment for proper post-level averaging
    df_daily_user = df_daily_user.copy()
    df_daily_user['weighted_sentiment'] = (df_daily_user['mean_sentiment'] * 
                                          df_daily_user['post_count'])
    
    # Group by date and calculate aggregations
    daily_stats = df_daily_user.groupby('date').agg({
        'post_count': 'sum',                    # Total posts per day
        'mean_sentiment': 'mean',               # Mean sentiment across users
        'weighted_sentiment': 'sum',            # For weighted average
    }).rename(columns={
        'post_count': 'total_posts_per_day',
        'mean_sentiment': 'mean_sentiment_per_user'
    })
    
    # Calculate weighted average sentiment across all posts
    daily_stats['mean_sentiment_per_post'] = (
        daily_stats['weighted_sentiment'] / daily_stats['total_posts_per_day']
    )
    
    # Clean up temporary column
    daily_stats = daily_stats.drop('weighted_sentiment', axis=1)
    
    return daily_stats


def safe_sample_for_visualization(df, max_samples=5000, random_state=42):
    """
    Safely sample DataFrame for visualization to avoid memory issues.
    Critical for t-SNE and UMAP which can fail with large datasets.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        max_samples (int): Maximum number of samples
        random_state (int): Random seed for reproducibility
        
    Returns:
        pd.DataFrame: Sampled DataFrame
    """
    if len(df) <= max_samples:
        return df
    
    sampled = df.sample(n=max_samples, random_state=random_state)
    print(f"âš ï¸  Sampled {max_samples} rows from {len(df)} for visualization")
    return sampled


def create_feature_summary(df, feature_cols):
    """
    Create comprehensive summary statistics for selected features.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        feature_cols (list): List of feature columns to summarize
        
    Returns:
        pd.DataFrame: Summary statistics
    """
    summary = df[feature_cols].describe()
    summary.loc['missing'] = df[feature_cols].isnull().sum()
    summary.loc['missing_pct'] = (df[feature_cols].isnull().sum() / len(df) * 100)
    
    return summary.round(4)## Utility Functions

Diese wiederverwendbaren Funktionen verbessern die Code-QualitÃ¤t und reduzieren Duplikationen:{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Klausur Data Science I\n",
    "### Klausur I im Sommersemester 2025\n",
    "\n",
    "**Optimierte und strukturierte Version**\n",
    "\n",
    "---\n",
    "\n",
    "## Allgemeine Informationen\n",
    "\n",
    "* Sie haben eine Woche Zeit, um die PrÃ¼fung abzuschlieÃŸen.\n",
    "* Sie kÃ¶nnen alle Quellen frei verwenden (einschlieÃŸlich ChatGPT oder Ã¤hnlicher Software).\n",
    "* Sie sollten die folgenden Pakete verwenden: `numpy, pandas, scipy, scikit-learn/sklearn, matplotlib, seaborn, statsmodels` und die nativen Bibliotheken von Python.\n",
    "* Der Code muss ausreichend kommentiert sein, um verstÃ¤ndlich zu sein. Schreiben Sie Funktionen, wenn Sie Code wiederverwenden.\n",
    "* BegrÃ¼nden Sie Entscheidungen bezÃ¼glich der Wahl von Plots, Hypothesentests usw. immer schriftlich und interpretieren Sie Ihre Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.signal import periodogram\n",
    "from scipy.stats import zscore, mannwhitneyu\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.tree import plot_tree, export_text, DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for consistent visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}