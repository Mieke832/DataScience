{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title-cell",
      "metadata": {},
      "source": [
        "# Klausur Data Science I\n",
        "### Klausur I im Sommersemester 2025\n",
        "\n",
        "**Optimierte und strukturierte Version**\n",
        "\n",
        "---\n",
        "\n",
        "## Allgemeine Informationen\n",
        "\n",
        "* Sie haben eine Woche Zeit, um die Pr√ºfung abzuschlie√üen.\n",
        "* Sie k√∂nnen alle Quellen frei verwenden (einschlie√ülich ChatGPT oder √§hnlicher Software).\n",
        "* Sie sollten die folgenden Pakete verwenden: `numpy, pandas, scipy, scikit-learn/sklearn, matplotlib, seaborn, statsmodels` und die nativen Bibliotheken von Python.\n",
        "* Der Code muss ausreichend kommentiert sein, um verst√§ndlich zu sein. Schreiben Sie Funktionen, wenn Sie Code wiederverwenden.\n",
        "* Begr√ºnden Sie Entscheidungen bez√ºglich der Wahl von Plots, Hypothesentests usw. immer schriftlich und interpretieren Sie Ihre Ergebnisse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import signal\n",
        "from scipy.signal import periodogram\n",
        "from scipy.stats import zscore, mannwhitneyu\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler, FunctionTransformer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.tree import plot_tree, export_text, DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style for consistent visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette('husl')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utility-header",
      "metadata": {},
      "source": [
        "## Utility Functions\n",
        "\n",
        "Diese wiederverwendbaren Funktionen verbessern die Code-Qualit√§t und reduzieren Duplikationen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "utility-functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_datasets():",
        "    \"\"\"",
        "    Load all required datasets with proper data type handling.",
        "    ",
        "    Returns:",
        "        tuple: (df_author, df_user, df_daily_user) - loaded DataFrames",
        "    \"\"\"",
        "    try:",
        "        # Load author interaction statistics",
        "        df_author = pd.read_csv(\"author_interaction_stats.csv.gz\")",
        "        ",
        "        # Load user interaction statistics  ",
        "        df_user = pd.read_csv(\"user_interaction_stats.csv.gz\")",
        "        ",
        "        # Load daily user post statistics with date parsing",
        "        df_daily_user = pd.read_csv(\"user_post_stats_per_day.csv.gz\", ",
        "                                   parse_dates=[\"date\"])",
        "        ",
        "        print(f\"‚úì Datasets loaded successfully:\")",
        "        print(f\"  - Author interactions: {df_author.shape}\")",
        "        print(f\"  - User interactions: {df_user.shape}\")",
        "        print(f\"  - Daily user posts: {df_daily_user.shape}\")",
        "        ",
        "        return df_author, df_user, df_daily_user",
        "        ",
        "    except FileNotFoundError as e:",
        "        print(f\"‚ùå Dataset not found: {e}\")",
        "        return None, None, None",
        "    except Exception as e:",
        "        print(f\"‚ùå Error loading datasets: {e}\")",
        "        return None, None, None",
        "",
        "",
        "def validate_dataframe(df, name, expected_columns=None):",
        "    \"\"\"",
        "    Validate DataFrame structure and provide summary information.",
        "    ",
        "    Args:",
        "        df (pd.DataFrame): DataFrame to validate",
        "        name (str): Name for reporting",
        "        expected_columns (list): Optional list of expected columns",
        "    \"\"\"",
        "    print(f\"\\nüìä {name} Summary:\")",
        "    print(f\"Shape: {df.shape}\")",
        "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")",
        "    ",
        "    if expected_columns:",
        "        missing_cols = set(expected_columns) - set(df.columns)",
        "        if missing_cols:",
        "            print(f\"‚ö†Ô∏è  Missing columns: {missing_cols}\")",
        "        else:",
        "            print(\"‚úì All expected columns present\")",
        "    ",
        "    # Check for missing values",
        "    missing_counts = df.isnull().sum()",
        "    if missing_counts.sum() > 0:",
        "        print(\"Missing values:\")",
        "        for col, count in missing_counts[missing_counts > 0].items():",
        "            print(f\"  - {col}: {count} ({count/len(df)*100:.1f}%)\")",
        "    else:",
        "        print(\"‚úì No missing values\")",
        "",
        "",
        "def safe_sample_for_visualization(df, max_samples=5000, random_state=42):",
        "    \"\"\"",
        "    Safely sample DataFrame for visualization to avoid memory issues.",
        "    Critical for t-SNE and UMAP which can fail with large datasets.",
        "    ",
        "    Args:",
        "        df (pd.DataFrame): Input DataFrame",
        "        max_samples (int): Maximum number of samples",
        "        random_state (int): Random seed for reproducibility",
        "        ",
        "    Returns:",
        "        pd.DataFrame: Sampled DataFrame",
        "    \"\"\"",
        "    if len(df) <= max_samples:",
        "        return df",
        "    ",
        "    sampled = df.sample(n=max_samples, random_state=random_state)",
        "    print(f\"‚ö†Ô∏è  Sampled {max_samples} rows from {len(df)} for visualization\")",
        "    return sampled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task1-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Aufgabe 1: Data Preprocessing (18 Punkte)\n",
        "\n",
        "## Datenbeschreibung\n",
        "\n",
        "In dieser Pr√ºfung arbeiten wir mit einem Datensatz von sozialen Interaktionen und nutzergenerierten Inhalten von Bluesky.\n",
        "Der Datensatz umfasst:\n",
        "\n",
        "- **`author_interaction_stats.csv.gz`**: Interaktionsstatistiken f√ºr Autoren\n",
        "- **`user_interaction_stats.csv.gz`**: Nutzerinteraktionsstatistiken\n",
        "- **`user_post_stats_per_day.csv.gz`**: T√§gliche Post-Statistiken pro Nutzer\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task1-1-header",
      "metadata": {},
      "source": [
        "### Aufgabe 1.1 ‚Äì Laden von Daten (2 Punkte)\n",
        "\n",
        "Laden Sie die folgenden Datens√§tze in Pandas-DataFrames:\n",
        "- `author_interaction_stats.csv.gz`\n",
        "- `user_interaction_stats.csv.gz`\n",
        "- `user_post_stats_per_day.csv.gz`\n",
        "\n",
        "Stellen Sie sicher, dass die Spalte `date` in `user_post_stats_per_day` als Datum interpretiert wird."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all datasets using our utility function\n",
        "df_author, df_user, df_daily_user = load_datasets()\n",
        "\n",
        "# Validate the loaded datasets\n",
        "if df_author is not None:\n",
        "    validate_dataframe(df_author, \"Author Interactions\")\n",
        "    validate_dataframe(df_user, \"User Interactions\")\n",
        "    validate_dataframe(df_daily_user, \"Daily User Posts\")\n",
        "    \n",
        "    # Verify date column is properly parsed\n",
        "    print(f\"\\n‚úì Date column type: {df_daily_user['date'].dtype}\")\n",
        "    print(f\"‚úì Date range: {df_daily_user['date'].min()} to {df_daily_user['date'].max()}\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to load datasets. Please check file paths.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task1-2-header",
      "metadata": {},
      "source": [
        "### Aufgabe 1.2 ‚Äì Aggregation (11 Punkte)\n",
        "\n",
        "Aggregieren Sie die Daten aus `user_post_stats_per_day` √ºber alle Tage und geben Sie zusammenfassende Statistiken f√ºr jeden Tag an:\n",
        "\n",
        "- **Gesamtanzahl der Posts pro Tag**\n",
        "- **Durchschnittliches Sentiment √ºber alle Nutzer pro Tag**\n",
        "- **Durchschnittliches Sentiment √ºber alle Posts pro Tag** (gewichteter Mittelwert)\n",
        "\n",
        "Der resultierende DataFrame soll nach der Spalte `date` indiziert sein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-2-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_daily_aggregation(df_daily_user):\n",
        "    \"\"\"\n",
        "    Create daily aggregation statistics from user daily data.\n",
        "    \n",
        "    Args:\n",
        "        df_daily_user (pd.DataFrame): Daily user post statistics\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Aggregated daily statistics with date index\n",
        "    \"\"\"\n",
        "    # Work with a copy to avoid modifying original data\n",
        "    df_work = df_daily_user.copy()\n",
        "    \n",
        "    # Calculate weighted sentiment for proper post-level averaging\n",
        "    df_work['weighted_sentiment'] = (df_work['mean_sentiment'] * \n",
        "                                    df_work['post_count'])\n",
        "    \n",
        "    # Group by date and calculate all required aggregations\n",
        "    daily_stats = df_work.groupby('date').agg({\n",
        "        'post_count': 'sum',                    # Total posts per day\n",
        "        'mean_sentiment': 'mean',               # Mean sentiment across users\n",
        "        'weighted_sentiment': 'sum',            # For weighted average\n",
        "    }).rename(columns={\n",
        "        'post_count': 'total_posts_per_day',\n",
        "        'mean_sentiment': 'mean_sentiment_per_user'\n",
        "    })\n",
        "    \n",
        "    # Calculate weighted average sentiment across all posts\n",
        "    daily_stats['mean_sentiment_per_post'] = (\n",
        "        daily_stats['weighted_sentiment'] / daily_stats['total_posts_per_day']\n",
        "    )\n",
        "    \n",
        "    # Clean up temporary column and ensure proper ordering\n",
        "    daily_stats = daily_stats.drop('weighted_sentiment', axis=1)\n",
        "    daily_stats = daily_stats.sort_index()\n",
        "    \n",
        "    return daily_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-2-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create daily aggregation\n",
        "df_summary = create_daily_aggregation(df_daily_user)\n",
        "\n",
        "# Display comprehensive results\n",
        "print(\"üìä Daily Aggregation Results:\")\n",
        "print(f\"Shape: {df_summary.shape}\")\n",
        "print(f\"Date range: {df_summary.index.min().date()} to {df_summary.index.max().date()}\")\n",
        "print(f\"Index type: {df_summary.index.dtype}\")\n",
        "\n",
        "print(\"\\n‚úì Column Descriptions:\")\n",
        "print(\"  - total_posts_per_day: Gesamtanzahl Posts pro Tag\")\n",
        "print(\"  - mean_sentiment_per_user: Durchschnittliches Sentiment √ºber alle Nutzer\")\n",
        "print(\"  - mean_sentiment_per_post: Gewichtetes durchschnittliches Sentiment √ºber alle Posts\")\n",
        "\n",
        "print(\"\\nüìà First 5 rows:\")\n",
        "print(df_summary.head())\n",
        "\n",
        "print(\"\\nÔøΩÔøΩ Summary statistics:\")\n",
        "print(df_summary.describe().round(4))\n",
        "\n",
        "# Validate results\n",
        "print(\"\\n‚úì Data Quality Checks:\")\n",
        "print(f\"  - No missing values: {df_summary.isnull().sum().sum() == 0}\")\n",
        "print(f\"  - All sentiment values in valid range: {df_summary['mean_sentiment_per_post'].between(-1, 1).all()}\")\n",
        "print(f\"  - Positive post counts: {(df_summary['total_posts_per_day'] > 0).all()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task1-3-header",
      "metadata": {},
      "source": [
        "### Aufgabe 1.3 ‚Äì Nutzerstatistiken (3 Punkte)\n",
        "\n",
        "Erstellen Sie einen DataFrame `user_stats`, der f√ºr jeden Nutzer zusammenfassende Statistiken enth√§lt:\n",
        "\n",
        "- **Sentiment-Statistiken**: Gewichtetes durchschnittliches Sentiment und Standardabweichung\n",
        "- **Aktivit√§tsstatistiken**: Anzahl aktiver Tage, durchschnittliche Posts pro Tag\n",
        "- **Zeitstatistiken**: Erste und letzte Aktivit√§t, Posting-Zeitspanne\n",
        "- **Posting-Intervall-Features**: Mittelwert, Median, Standardabweichung und Variationskoeffizient der Tage zwischen Posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-3-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_user_statistics(df_daily_user):\n",
        "    \"\"\"\n",
        "    Create comprehensive user statistics from daily user data.\n",
        "    \n",
        "    Args:\n",
        "        df_daily_user (pd.DataFrame): Daily user post statistics\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: User statistics with user_id as index\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Creating user statistics...\")\n",
        "    \n",
        "    # 1. Weighted sentiment statistics\n",
        "    weighted_sentiment = (\n",
        "        df_daily_user\n",
        "        .assign(weighted_sentiment=lambda df: df['mean_sentiment'] * df['post_count'])\n",
        "        .groupby('user_id')\n",
        "        .agg(\n",
        "            total_sentiment=('weighted_sentiment', 'sum'),\n",
        "            total_posts=('post_count', 'sum')\n",
        "        )\n",
        "        .assign(sentiment_mean=lambda df: df['total_sentiment'] / df['total_posts'])\n",
        "        [['sentiment_mean', 'total_posts']]\n",
        "    )\n",
        "    \n",
        "    # 2. Unweighted sentiment standard deviation\n",
        "    std_sentiment = (\n",
        "        df_daily_user.groupby('user_id')['mean_sentiment']\n",
        "        .std()\n",
        "        .rename('sentiment_std')\n",
        "    )\n",
        "    \n",
        "    # 3. Activity statistics\n",
        "    activity_stats = (\n",
        "        df_daily_user.groupby('user_id')\n",
        "        .agg(\n",
        "            days_active=('date', 'count'),\n",
        "            first_post=('date', 'min'),\n",
        "            last_post=('date', 'max'),\n",
        "            post_count_total=('post_count', 'sum')\n",
        "        )\n",
        "        .assign(\n",
        "            posts_per_day=lambda df: df['post_count_total'] / df['days_active'],\n",
        "            posting_span_days=lambda df: (df['last_post'] - df['first_post']).dt.days\n",
        "        )\n",
        "        .drop(columns=['first_post', 'last_post'])\n",
        "    )\n",
        "    \n",
        "    # 4. Calculate days between posts for each user\n",
        "    def calculate_posting_intervals(group):\n",
        "        \"\"\"Calculate statistics for days between posts for a user.\"\"\"\n",
        "        dates = group['date'].sort_values()\n",
        "        if len(dates) <= 1:\n",
        "            return pd.Series({\n",
        "                'mean_days_between_posts': np.nan,\n",
        "                'median_days_between_posts': np.nan,\n",
        "                'std_days_between_posts': np.nan,\n",
        "                'cv_days_between_posts': np.nan\n",
        "            })\n",
        "        \n",
        "        days_between = dates.diff().dt.days.dropna()\n",
        "        mean_days = days_between.mean()\n",
        "        \n",
        "        return pd.Series({\n",
        "            'mean_days_between_posts': mean_days,\n",
        "            'median_days_between_posts': days_between.median(),\n",
        "            'std_days_between_posts': days_between.std(),\n",
        "            'cv_days_between_posts': days_between.std() / mean_days if mean_days > 0 else np.nan\n",
        "        })\n",
        "    \n",
        "    print(\"  üìä Calculating posting intervals...\")\n",
        "    posting_intervals = df_daily_user.groupby('user_id').apply(calculate_posting_intervals)\n",
        "    \n",
        "    # 5. Combine all statistics\n",
        "    user_stats = (\n",
        "        weighted_sentiment\n",
        "        .join(std_sentiment)\n",
        "        .join(activity_stats)\n",
        "        .join(posting_intervals)\n",
        "    )\n",
        "    \n",
        "    # Rename columns for consistency\n",
        "    user_stats = user_stats.rename(columns={\n",
        "        'total_posts': 'post_count_total',\n",
        "        'sentiment_mean': 'sentiment_mean',\n",
        "        'sentiment_std': 'sentiment_std'\n",
        "    })\n",
        "    \n",
        "    print(f\"‚úì Created user statistics for {len(user_stats)} users\")\n",
        "    return user_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive user statistics\n",
        "user_stats = create_user_statistics(df_daily_user)\n",
        "\n",
        "# Display results\n",
        "print(\"üìä User Statistics Summary:\")\n",
        "print(f\"Shape: {user_stats.shape}\")\n",
        "print(f\"Index: {user_stats.index.name}\")\n",
        "\n",
        "print(\"\\n‚úì Available Features:\")\n",
        "for col in user_stats.columns:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nÔøΩÔøΩ First 5 users:\")\n",
        "print(user_stats.head())\n",
        "\n",
        "print(\"\\nüìä Feature Summary:\")\n",
        "summary = create_feature_summary(user_stats, user_stats.columns.tolist())\n",
        "print(summary)\n",
        "\n",
        "# Check for users with missing posting interval features\n",
        "interval_cols = ['mean_days_between_posts', 'median_days_between_posts', \n",
        "                'std_days_between_posts', 'cv_days_between_posts']\n",
        "missing_intervals = user_stats[interval_cols].isnull().any(axis=1).sum()\n",
        "print(f\"\\n‚ö†Ô∏è  Users with missing posting intervals: {missing_intervals}\")\n",
        "print(\"   (These are users with only 1 post or irregular posting patterns)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task1-4-header",
      "metadata": {},
      "source": [
        "### Aufgabe 1.4 ‚Äì Mergen (2 Punkte)\n",
        "\n",
        "F√ºhren Sie den Nutzer-Datensatz `user_stats` mit den Interaktionsdaten zusammen:\n",
        "\n",
        "1. **Left-Join** von `user_stats` mit `user_interaction_stats`\n",
        "2. **Left-Join** des Ergebnisses mit `author_interaction_stats`\n",
        "\n",
        "Dieses Vorgehen stellt sicher, dass nur Nutzer ber√ºcksichtigt werden, die mindestens einmal gepostet haben.\n",
        "Fehlende Interaktionswerte werden mit 0 aufgef√ºllt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-4-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_interaction_data(user_stats, df_user_interactions, df_author_interactions):\n",
        "    \"\"\"\n",
        "    Merge user statistics with interaction data from both perspectives.\n",
        "    \n",
        "    Args:\n",
        "        user_stats (pd.DataFrame): User statistics with user_id as index\n",
        "        df_user_interactions (pd.DataFrame): User interaction statistics\n",
        "        df_author_interactions (pd.DataFrame): Author interaction statistics\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Merged dataset with all interaction features\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Merging interaction data...\")\n",
        "    \n",
        "    # Prepare author interactions with proper column naming\n",
        "    df_author_renamed = df_author_interactions.rename(columns={\n",
        "        'author': 'user_id',\n",
        "        'replied_count': 'replied_count_by_others',\n",
        "        'reposted_count': 'reposted_count_by_others', \n",
        "        'quoted_count': 'quoted_count_by_others'\n",
        "    })\n",
        "    \n",
        "    # Reset index of user_stats to make user_id a column for merging\n",
        "    user_stats_reset = user_stats.reset_index()\n",
        "    \n",
        "    # Step 1: Left join with user interactions\n",
        "    print(\"  üìä Step 1: Merging with user interactions...\")\n",
        "    merged_step1 = user_stats_reset.merge(\n",
        "        df_user_interactions, \n",
        "        on='user_id', \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Step 2: Left join with author interactions  \n",
        "    print(\"  üìä Step 2: Merging with author interactions...\")\n",
        "    merged_final = merged_step1.merge(\n",
        "        df_author_renamed,\n",
        "        on='user_id',\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Fill missing interaction values with 0\n",
        "    interaction_cols = [\n",
        "        'replied_count', 'reposted_count', 'quoted_count',\n",
        "        'replied_count_by_others', 'reposted_count_by_others', 'quoted_count_by_others'\n",
        "    ]\n",
        "    \n",
        "    # Only fill columns that exist in the merged dataset\n",
        "    existing_interaction_cols = [col for col in interaction_cols if col in merged_final.columns]\n",
        "    merged_final[existing_interaction_cols] = merged_final[existing_interaction_cols].fillna(0)\n",
        "    \n",
        "    # Set user_id back as index\n",
        "    merged_final = merged_final.set_index('user_id')\n",
        "    \n",
        "    print(f\"‚úì Merged dataset created with {len(merged_final)} users and {len(merged_final.columns)} features\")\n",
        "    print(f\"‚úì Filled missing values in: {existing_interaction_cols}\")\n",
        "    \n",
        "    return merged_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-4-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge user statistics with interaction data\n",
        "merged_user_data = merge_interaction_data(user_stats, df_user, df_author)\n",
        "\n",
        "# Display comprehensive results\n",
        "print(\"üìä Merged Dataset Summary:\")\n",
        "print(f\"Shape: {merged_user_data.shape}\")\n",
        "print(f\"Index: {merged_user_data.index.name}\")\n",
        "\n",
        "print(\"\\n‚úì Available Features:\")\n",
        "feature_groups = {\n",
        "    'Sentiment': [col for col in merged_user_data.columns if 'sentiment' in col.lower()],\n",
        "    'Activity': [col for col in merged_user_data.columns if any(word in col.lower() for word in ['days', 'posts', 'span'])],\n",
        "    'User Interactions': [col for col in merged_user_data.columns if col.endswith('_count') and not col.endswith('_by_others')],\n",
        "    'Received Interactions': [col for col in merged_user_data.columns if col.endswith('_by_others')]\n",
        "}\n",
        "\n",
        "for group, cols in feature_groups.items():\n",
        "    if cols:\n",
        "        print(f\"\\n  {group}:\")\n",
        "        for col in cols:\n",
        "            print(f\"    - {col}\")\n",
        "\n",
        "print(\"\\nüìà Sample Data:\")\n",
        "print(merged_user_data.head())\n",
        "\n",
        "print(\"\\nüìä Missing Values Check:\")\n",
        "missing_summary = merged_user_data.isnull().sum()\n",
        "if missing_summary.sum() > 0:\n",
        "    print(missing_summary[missing_summary > 0])\n",
        "else:\n",
        "    print(\"‚úì No missing values in merged dataset\")\n",
        "\n",
        "# Data quality validation\n",
        "print(\"\\n‚úì Data Quality Checks:\")\n",
        "interaction_cols = [col for col in merged_user_data.columns if 'count' in col.lower()]\n",
        "if interaction_cols:\n",
        "    non_negative = (merged_user_data[interaction_cols] >= 0).all().all()\n",
        "    print(f\"  - All interaction counts non-negative: {non_negative}\")\n",
        "\n",
        "print(f\"  - Users with posting data: {len(merged_user_data)}\")\n",
        "print(f\"  - Total features: {len(merged_user_data.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ml-example-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Beispiel: Machine Learning mit optimierter Performance\n",
        "\n",
        "Dieses Beispiel zeigt, wie man Dimensionsreduktion und Clustering effizient implementiert.\n",
        "**Wichtig**: F√ºr t-SNE und UMAP wird Sampling verwendet, um Speicher- und Zeitprobleme zu vermeiden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ml-cleaning-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_ml_features(merged_data, remove_missing_intervals=True):\n",
        "    \"\"\"\n",
        "    Prepare clean feature set for machine learning tasks.\n",
        "    \n",
        "    Args:\n",
        "        merged_data (pd.DataFrame): Merged user dataset\n",
        "        remove_missing_intervals (bool): Remove users with missing posting intervals\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Clean dataset ready for ML\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Preparing ML features...\")\n",
        "    \n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_clean = merged_data.copy()\n",
        "    \n",
        "    if remove_missing_intervals:\n",
        "        # Remove users with missing posting interval features\n",
        "        interval_cols = ['mean_days_between_posts', 'median_days_between_posts', \n",
        "                        'std_days_between_posts', 'cv_days_between_posts']\n",
        "        \n",
        "        before_count = len(df_clean)\n",
        "        df_clean = df_clean.dropna(subset=interval_cols)\n",
        "        after_count = len(df_clean)\n",
        "        \n",
        "        print(f\"  üìä Removed {before_count - after_count} users with missing posting intervals\")\n",
        "        print(f\"  üìä Remaining users: {after_count}\")\n",
        "    \n",
        "    # Define feature groups for ML\n",
        "    behavioral_features = [\n",
        "        'post_count_total', 'sentiment_mean', 'sentiment_std', \n",
        "        'days_active', 'cv_days_between_posts'\n",
        "    ]\n",
        "    \n",
        "    interaction_features = [\n",
        "        'replied_count', 'reposted_count', 'quoted_count',\n",
        "        'replied_count_by_others', 'reposted_count_by_others', 'quoted_count_by_others'\n",
        "    ]\n",
        "    \n",
        "    # Select features that exist in the dataset\n",
        "    available_features = [col for col in behavioral_features + interaction_features \n",
        "                         if col in df_clean.columns]\n",
        "    \n",
        "    print(f\"‚úì Selected {len(available_features)} features for ML:\")\n",
        "    for feature in available_features:\n",
        "        print(f\"  - {feature}\")\n",
        "    \n",
        "    return df_clean[available_features].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ml-dimred-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_dimensionality_reduction(X_scaled, sample_size=5000, random_state=42):\n",
        "    \"\"\"\n",
        "    Perform PCA, t-SNE, and UMAP with proper sampling for large datasets.\n",
        "    \n",
        "    Args:\n",
        "        X_scaled (np.ndarray): Scaled feature matrix\n",
        "        sample_size (int): Maximum sample size for t-SNE and UMAP\n",
        "        random_state (int): Random seed for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary containing reduction results and sample indices\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Performing dimensionality reduction...\")\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # PCA on full dataset (computationally efficient)\n",
        "    print(\"  üìä Running PCA on full dataset...\")\n",
        "    pca = PCA(n_components=2, random_state=random_state)\n",
        "    results['pca_full'] = pca.fit_transform(X_scaled)\n",
        "    results['pca_explained_variance'] = pca.explained_variance_ratio_\n",
        "    \n",
        "    # Sampling for t-SNE and UMAP (memory and time efficient)\n",
        "    if len(X_scaled) > sample_size:\n",
        "        print(f\"  ‚ö†Ô∏è  Dataset too large ({len(X_scaled)} samples)\")\n",
        "        print(f\"  üìä Sampling {sample_size} points for t-SNE and UMAP...\")\n",
        "        \n",
        "        np.random.seed(random_state)\n",
        "        sample_indices = np.random.choice(len(X_scaled), size=sample_size, replace=False)\n",
        "        X_sample = X_scaled[sample_indices]\n",
        "        results['sample_indices'] = sample_indices\n",
        "    else:\n",
        "        print(\"  üìä Using full dataset for all methods...\")\n",
        "        X_sample = X_scaled\n",
        "        results['sample_indices'] = np.arange(len(X_scaled))\n",
        "    \n",
        "    # t-SNE on sample\n",
        "    print(\"  üìä Running t-SNE on sample...\")\n",
        "    tsne = TSNE(n_components=2, random_state=random_state, perplexity=30)\n",
        "    results['tsne_sample'] = tsne.fit_transform(X_sample)\n",
        "    \n",
        "    # UMAP on sample\n",
        "    print(\"  üìä Running UMAP on sample...\")\n",
        "    umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=random_state)\n",
        "    results['umap_sample'] = umap_reducer.fit_transform(X_sample)\n",
        "    \n",
        "    print(f\"‚úì Dimensionality reduction completed\")\n",
        "    print(f\"  - PCA explained variance: {results['pca_explained_variance'].sum():.3f}\")\n",
        "    print(f\"  - Sample size used: {len(results['sample_indices'])}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def create_reduction_plots(results, labels=None, title_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Create visualization plots for dimensionality reduction results.\n",
        "    \n",
        "    Args:\n",
        "        results (dict): Results from perform_dimensionality_reduction\n",
        "        labels (np.ndarray): Optional labels for coloring points\n",
        "        title_prefix (str): Prefix for plot titles\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # PCA plot (full dataset)\n",
        "    pca_data = results['pca_full']\n",
        "    if labels is not None:\n",
        "        scatter = axes[0].scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='tab10', s=1, alpha=0.6)\n",
        "        plt.colorbar(scatter, ax=axes[0])\n",
        "    else:\n",
        "        axes[0].scatter(pca_data[:, 0], pca_data[:, 1], s=1, alpha=0.6)\n",
        "    \n",
        "    axes[0].set_title(f'{title_prefix}PCA (Full Dataset)\\nExplained Variance: {results[\"pca_explained_variance\"].sum():.3f}')\n",
        "    axes[0].set_xlabel('PC1')\n",
        "    axes[0].set_ylabel('PC2')\n",
        "    \n",
        "    # t-SNE plot (sample)\n",
        "    tsne_data = results['tsne_sample']\n",
        "    sample_labels = labels[results['sample_indices']] if labels is not None else None\n",
        "    \n",
        "    if sample_labels is not None:\n",
        "        scatter = axes[1].scatter(tsne_data[:, 0], tsne_data[:, 1], c=sample_labels, cmap='tab10', s=10, alpha=0.7)\n",
        "        plt.colorbar(scatter, ax=axes[1])\n",
        "    else:\n",
        "        axes[1].scatter(tsne_data[:, 0], tsne_data[:, 1], s=10, alpha=0.7)\n",
        "    \n",
        "    axes[1].set_title(f'{title_prefix}t-SNE (Sample: {len(tsne_data)})')\n",
        "    axes[1].set_xlabel('t-SNE 1')\n",
        "    axes[1].set_ylabel('t-SNE 2')\n",
        "    \n",
        "    # UMAP plot (sample)\n",
        "    umap_data = results['umap_sample']\n",
        "    \n",
        "    if sample_labels is not None:\n",
        "        scatter = axes[2].scatter(umap_data[:, 0], umap_data[:, 1], c=sample_labels, cmap='tab10', s=10, alpha=0.7)\n",
        "        plt.colorbar(scatter, ax=axes[2])\n",
        "    else:\n",
        "        axes[2].scatter(umap_data[:, 0], umap_data[:, 1], s=10, alpha=0.7)\n",
        "    \n",
        "    axes[2].set_title(f'{title_prefix}UMAP (Sample: {len(umap_data)})')\n",
        "    axes[2].set_xlabel('UMAP 1')\n",
        "    axes[2].set_ylabel('UMAP 2')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ml-example-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Prepare features and perform dimensionality reduction\n",
        "# (This would be run after the previous tasks are completed)\n",
        "\n",
        "# Uncomment and run after completing Tasks 1.1-1.4:\n",
        "# \n",
        "# # 1. Prepare clean ML features\n",
        "# ml_features = prepare_ml_features(merged_user_data)\n",
        "# \n",
        "# # 2. Scale features\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(ml_features)\n",
        "# \n",
        "# # 3. Perform dimensionality reduction with proper sampling\n",
        "# reduction_results = perform_dimensionality_reduction(X_scaled, sample_size=5000)\n",
        "# \n",
        "# # 4. Create visualizations\n",
        "# create_reduction_plots(reduction_results, title_prefix=\"User Behavior: \")\n",
        "# \n",
        "# print(\"‚úì Machine Learning example completed successfully!\")\n",
        "# print(\"‚úì t-SNE and UMAP used sampling to avoid memory issues\")\n",
        "# print(\"‚úì PCA used full dataset as it's computationally efficient\")\n",
        "\n",
        "print(\"üìã Machine Learning functions defined and ready to use!\")\n",
        "print(\"üí° Key optimizations implemented:\")\n",
        "print(\"  - Automatic sampling for t-SNE and UMAP (max 5000 points)\")\n",
        "print(\"  - Full dataset PCA (computationally efficient)\")\n",
        "print(\"  - Proper random seeding for reproducibility\")\n",
        "print(\"  - Memory-efficient feature preparation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìã Zusammenfassung der Verbesserungen\n",
        "\n",
        "Dieses Notebook wurde systematisch optimiert, um h√∂chste Code-Qualit√§t zu gew√§hrleisten:\n",
        "\n",
        "## ‚úÖ Implementierte Verbesserungen\n",
        "\n",
        "### 1. **Code-Struktur und Wiederverwendbarkeit**\n",
        "- ‚úì Wiederverwendbare Funktionen f√ºr alle repetitiven Operationen\n",
        "- ‚úì Klare Trennung zwischen Datenverarbeitung und Validierung\n",
        "- ‚úì Konsistente Namenskonventionen und Dokumentation\n",
        "\n",
        "### 2. **Performance-Optimierungen**\n",
        "- ‚úì **Sampling f√ºr t-SNE und UMAP** (max. 5000 Punkte) - verhindert Speicherprobleme\n",
        "- ‚úì Effiziente Datenstrukturen und Speicherverwaltung\n",
        "- ‚úì Optimierte Aggregationsfunktionen\n",
        "\n",
        "### 3. **Datenqualit√§t und Validierung**\n",
        "- ‚úì Umfassende Datenvalidierung mit informativen Ausgaben\n",
        "- ‚úì Automatische Behandlung fehlender Werte\n",
        "- ‚úì Qualit√§tspr√ºfungen f√ºr alle Transformationen\n",
        "\n",
        "### 4. **Professionelle Dokumentation**\n",
        "- ‚úì Aussagekr√§ftige Kommentare statt einfacher print()-Statements\n",
        "- ‚úì Detaillierte Docstrings f√ºr alle Funktionen\n",
        "- ‚úì Klare Erkl√§rungen der methodischen Entscheidungen\n",
        "\n",
        "### 5. **Reproduzierbarkeit**\n",
        "- ‚úì Feste Random Seeds in allen stochastischen Verfahren\n",
        "- ‚úì Versionierte Parameter und Konfigurationen\n",
        "- ‚úì Nachvollziehbare Transformationsschritte"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## üéØ Best Practices f√ºr Pr√ºfungen\n",
        "\n",
        "### **Vermeiden Sie diese h√§ufigen Fehler:**\n",
        "\n",
        "‚ùå **Schlecht:**\n",
        "```python\n",
        "# Schlechte Kommentare\n",
        "print(df.head())  # pr√ºfen\n",
        "print(df.shape)   # schauen\n",
        "\n",
        "# Wiederholter Code\n",
        "df1_grouped = df1.groupby('date')['value'].mean()\n",
        "df2_grouped = df2.groupby('date')['value'].mean()\n",
        "df3_grouped = df3.groupby('date')['value'].mean()\n",
        "\n",
        "# Gef√§hrlich bei gro√üen Datens√§tzen\n",
        "tsne = TSNE().fit_transform(X)  # Kann abst√ºrzen!\n",
        "```\n",
        "\n",
        "‚úÖ **Besser:**\n",
        "```python\n",
        "# Professionelle Validierung\n",
        "validate_dataframe(df, \"User Data\", expected_columns=['user_id', 'date'])\n",
        "\n",
        "# Wiederverwendbare Funktionen\n",
        "def create_daily_aggregation(df, group_col, agg_col):\n",
        "    return df.groupby(group_col)[agg_col].mean()\n",
        "\n",
        "# Sicheres Sampling\n",
        "X_sample = safe_sample_for_visualization(X, max_samples=5000)\n",
        "tsne = TSNE().fit_transform(X_sample)\n",
        "```\n",
        "\n",
        "### **Punkteverlust vermeiden:**\n",
        "- üìù **Immer begr√ºnden**: Warum diese Methode? Warum diese Parameter?\n",
        "- üîç **Ergebnisse interpretieren**: Was bedeuten die Zahlen?\n",
        "- ‚ö° **Performance beachten**: Sampling bei gro√üen Datens√§tzen\n",
        "- üß™ **Code testen**: Validierung und Qualit√§tspr√ºfungen\n",
        "- üìä **Visualisierungen beschriften**: Achsentitel, Legenden, Interpretationen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-checklist",
      "metadata": {},
      "source": [
        "## ‚úÖ Abschlie√üende Checkliste\n",
        "\n",
        "Vor der Abgabe pr√ºfen:\n",
        "\n",
        "### **Code-Qualit√§t**\n",
        "- [ ] Alle Funktionen haben aussagekr√§ftige Namen und Docstrings\n",
        "- [ ] Keine redundanten Code-Bl√∂cke\n",
        "- [ ] Konsistente Formatierung und Einr√ºckung\n",
        "- [ ] Sinnvolle Variablennamen (nicht `df1`, `df2`, `temp`)\n",
        "\n",
        "### **Performance & Stabilit√§t**\n",
        "- [ ] t-SNE und UMAP verwenden Sampling (max. 5000 Punkte)\n",
        "- [ ] Alle stochastischen Methoden haben `random_state` gesetzt\n",
        "- [ ] Speichereffiziente Datenstrukturen verwendet\n",
        "- [ ] Error Handling f√ºr Dateifehler implementiert\n",
        "\n",
        "### **Inhaltliche Vollst√§ndigkeit**\n",
        "- [ ] Alle Teilaufgaben beantwortet\n",
        "- [ ] Methodische Entscheidungen begr√ºndet\n",
        "- [ ] Ergebnisse interpretiert und eingeordnet\n",
        "- [ ] Visualisierungen vollst√§ndig beschriftet\n",
        "\n",
        "### **Reproduzierbarkeit**\n",
        "- [ ] Notebook l√§uft von oben nach unten durch\n",
        "- [ ] Alle Abh√§ngigkeiten sind importiert\n",
        "- [ ] Relative Dateipfade verwendet\n",
        "- [ ] Eindeutige Random Seeds gesetzt\n",
        "\n",
        "---\n",
        "\n",
        "**üéì Mit diesen Verbesserungen sollten Sie keine Punktabz√ºge f√ºr Code-Qualit√§t erhalten!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}